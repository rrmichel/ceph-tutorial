# Ceph 101

```
# ceph ...
# rbd ...
# radosgw-admin ...
# crushtool ...
```


# ceph status

`# ceph -s`
```
  cluster:
    id:     1b1a561a-e76d-497f-8677-5db6efeb275f
    health: HEALTH_OK

  services:
    mon:        1 daemons, quorum ceph-aio-1 (age 3h)
    mgr:        ceph-aio-1(active, since 3h)
    osd:        3 osds: 3 up (since 3h), 3 in (since 10w)

  data:
    pools:   6 pools, 80 pgs
    objects: 4.34k objects, 16 GiB
    usage:   51 GiB used, 42 GiB / 93 GiB avail
    pgs:     80 active+clean

  io:
    client:   2.3 KiB/s rd, 170 B/s wr, 2 op/s rd, 0 op/s wr
```


# ceph df

`# ceph df`
```
RAW STORAGE:
    CLASS     SIZE       AVAIL      USED       RAW USED     %RAW USED
    hdd       93 GiB     42 GiB     48 GiB       51 GiB         55.26
    TOTAL     93 GiB     42 GiB     48 GiB       51 GiB         55.26

POOLS:
    POOL                    ID     STORED      OBJECTS     USED        %USED     MAX AVAIL
    rbd                      8       804 B           6     577 KiB         0        12 GiB
    mirror                   9      16 GiB       4.12k      48 GiB     56.56        12 GiB
```


# ceph osd tree

`# ceph osd tree`
```
ID CLASS WEIGHT  TYPE NAME           STATUS REWEIGHT PRI-AFF
-1       0.09099 root default
-3       0.09099     host ceph-aio-1
 0   hdd 0.03000         osd.0           up  1.00000 1.00000
 1   hdd 0.03000         osd.1           up  1.00000 1.00000
 2   hdd 0.03000         osd.2           up  1.00000 1.00000
```


# systemd

```
# systemctl [status|start|stop] ...
  ceph-mon@$hostname
  ceph-osd@$id
  ceph-mgr@$hostname
  ceph[-mon|osd].target
```


# /etc/ceph/

```bash
# ls -l /etc/ceph/
-rw-r--r--. 1 root root 151 May  8 12:22 ceph.client.admin.keyring
-rw-r--r--. 1 root root 406 May 19 18:08 ceph.conf
-rw-r--r--. 1 root root  92 Sep 13 21:29 rbdmap
```


# Recap

```
# ceph -s
# ceph df
# ceph osd tree
# systemctl status ceph-mon@$hostname
# systemctl status ceph-osd@$id
```