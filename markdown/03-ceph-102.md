# Ceph 102


# Placement Groups

[PGCalc @ www.ceph.com](http://www.ceph.com/pgcalc)

```
             (OSDs * 100)
Total PGs =  ------------
              pool size
```


```
# ceph osd pool create rbd 32
```


```
# ceph osd lspools
# ceph osd pool ls [detail]
```


```
# ceph osd pool set <pool> <key> <value>
```


```none
size|min_size|pg_num|pgp_num|pgp_num_actual|crush_rule
hashpspool|nodelete|nopgchange|nosizechange|write_fadvise_dontneed
noscrub|nodeep-scrub|hit_set_type|hit_set_period|hit_set_count
hit_set_fpp|use_gmt_hitset|target_max_bytes|target_max_objects
cache_target_dirty_ratio|cache_target_dirty_high_ratio
cache_target_full_ratio|cache_min_flush_age|cache_min_evict_age
min_read_recency_for_promote|min_write_recency_for_promote
fast_read|hit_set_grade_decay_rate|hit_set_search_last_n
scrub_min_interval|scrub_max_interval|deep_scrub_interval
recovery_priority|recovery_op_priority|scrub_priority
compression_mode|compression_algorithm|compression_required_ratio
compression_max_blob_size|compression_min_blob_size|csum_type
csum_min_block|csum_max_block|allow_ec_overwrites
fingerprint_algorithm|pg_autoscale_mode|pg_autoscale_bias
pg_num_min|target_size_bytes|target_size_ratio
```


```
# ceph osd pool set rbd min_size 1
```


```
# ceph pg $id query
# ceph pg stats
# ceph pg ls-by-[pool|osd] ...
# ceph pg dump pgs_brief
```


```
# ceph osd pool create ec 32 erasure
# ceph osd erasure-code-profile get default

# ceph osd erasure-code-profile set ec-rack k=2 m=1 crush-failure-domain=rack
# ceph osd pool create ec2 32 erasure ec-rack
# ceph osd pool set <pool> crush_rule <id>

```


```
# ceph osd metadata ...
```


# ceph auth ...
# ceph crush ...