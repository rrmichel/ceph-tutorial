# Ceph 102


# Placement Groups

[PGCalc @ www.ceph.com](http://www.ceph.com/pgcalc)

```
             (OSDs * 100)
Total PGs =  ------------
              pool size
```


```
# ceph osd pool create rbd 32
```


```
# ceph osd lspools
# ceph osd pool ls [detail]
```


```
# ceph osd pool set <pool> <key> <value>
```


# 51 (!)

```none
size min_size pg_num pgp_num pgp_num_actual crush_rule
hashpspool nodelete nopgchange nosizechange write_fadvise_dontneed
noscrub nodeep-scrub hit_set_type hit_set_period hit_set_count
hit_set_fpp use_gmt_hitset target_max_bytes target_max_objects
cache_target_dirty_ratio cache_target_dirty_high_ratio
cache_target_full_ratio cache_min_flush_age cache_min_evict_age
min_read_recency_for_promote min_write_recency_for_promote
fast_read hit_set_grade_decay_rate hit_set_search_last_n
scrub_min_interval scrub_max_interval deep_scrub_interval
recovery_priority recovery_op_priority scrub_priority
compression_mode compression_algorithm compression_required_ratio
compression_max_blob_size compression_min_blob_size csum_type
csum_min_block csum_max_block allow_ec_overwrites
fingerprint_algorithm pg_autoscale_mode pg_autoscale_bias
pg_num_min target_size_bytes target_size_ratio
```


```
# ceph osd pool set rbd min_size 1
```


```
# ceph pg $pgid query
# ceph pg stat
# ceph pg ls-by-[pool|osd] ...
# ceph pg dump pgs_brief
# ceph pg repair $pgid
```


```
# ceph osd pool create ec 32 erasure
# ceph osd erasure-code-profile get default

# ceph osd erasure-code-profile set ec-rack k=2 m=1 \
  crush-failure-domain=rack
# ceph osd pool create ec2 32 erasure ec-rack
# ceph osd pool set <pool> crush_rule <id>
```


```
# ceph osd pool rm <pool> ...
```


# ceph config 

```
# ceph config ls
# ceph config set mon mon_allow_pool_delete true
# ceph osd pool rm <pool> ...

```


# ceph auth

```
# ceph auth list
```


```
# ceph auth get-or-create client.user \
  mon 'allow r' \
  osd 'allow r'
```


```
# ceph auth get-or-create client.user2 \
  mon 'allow r' \
  osd 'allow rw' \
  mds 'allow r, allow rw path=/backup'
```

```
# ceph fs authorize cephfs client.user3 / r /backup rw
```


```
# ceph auth get client.user
# ceph auth get-key client.user
```


```
# ceph auth caps client.user ....
```


# cephx profiles

* profile osd (Monitor only)
* profile mds (Monitor only)
* profile bootstrap-osd (Monitor only)
* profile bootstrap-mds (Monitor only)
* profile bootstrap-rbd (Monitor only)
* profile bootstrap-rbd-mirror (Monitor only)
* profile rbd (Monitor and OSD)
* profile rbd-mirror (Monitor only)
* profile rbd-read-only (OSD only)


```
# ceph auth get-or-create client.rbd \
  mon 'profile rbd' \
  osd 'profile rbd'
```


# ceph osd crush

```
# ceph osd crush add-bucket <name> <type>
# ceph osd crush add-bucket rack1 rack
```

```
# ceph osd crush move <name> <target>
# ceph osd crush move rack1 root=default
```


```
# ceph osd crush create-or-move <osd.id> <weight> <target>
# ceph osd crush create-or-move osd.1 4.00 host=ceph1
```


```
# ceph osd crush rule create-replicated \
  <name> <root> <type> {<class>}
# ceph osd crush rule create-replicated \
  rack-ssd default ssd
```


```
# ceph osd crush rm-device-class osd.<id>
# ceph osd crush set-device-class <new-class> osd.<id>
```